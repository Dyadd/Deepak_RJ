[
  {
    "objectID": "posts/What_Medicine_needs_to_get_right_about_AI/index.html",
    "href": "posts/What_Medicine_needs_to_get_right_about_AI/index.html",
    "title": "What Medicine needs to get right about AI",
    "section": "",
    "text": "The transformer architecture has revolutionized AI, enabling systems to capture complex non-linear relationships in vast datasets. In medicine, this has led to remarkable capabilities:\n\n\n\n\n\n\nCurrent Applications\n\n\n\n\nClinical Communication: When applied to medical language, AI systems now understand medical context and can answer patient questions at a level comparable to or exceeding doctors\nAdministrative Efficiency: When applied to human conversations, we can now automate clinical scribing and writing medical letters\nWorkflow Enhancement: When applied to the EMR, with text-to-action & computer use, you could even automate tedious EMR navigations.\nResearch Advancement: When applied to massive multi-omic biological data in data-rich fields like oncology, the next biomedical breakthroughs will be aided by AI foundation models.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "What Medicine needs to get right about AI"
    ]
  },
  {
    "objectID": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#the-ai-revolution-in-medicine-beyond-the-hype",
    "href": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#the-ai-revolution-in-medicine-beyond-the-hype",
    "title": "What Medicine needs to get right about AI",
    "section": "",
    "text": "The transformer architecture has revolutionized AI, enabling systems to capture complex non-linear relationships in vast datasets. In medicine, this has led to remarkable capabilities:\n\n\n\n\n\n\nCurrent Applications\n\n\n\n\nClinical Communication: When applied to medical language, AI systems now understand medical context and can answer patient questions at a level comparable to or exceeding doctors\nAdministrative Efficiency: When applied to human conversations, we can now automate clinical scribing and writing medical letters\nWorkflow Enhancement: When applied to the EMR, with text-to-action & computer use, you could even automate tedious EMR navigations.\nResearch Advancement: When applied to massive multi-omic biological data in data-rich fields like oncology, the next biomedical breakthroughs will be aided by AI foundation models.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "What Medicine needs to get right about AI"
    ]
  },
  {
    "objectID": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#the-implementation-challenge",
    "href": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#the-implementation-challenge",
    "title": "What Medicine needs to get right about AI",
    "section": "2 The Implementation Challenge",
    "text": "2 The Implementation Challenge\nWe clinicians will, or already are, using AI tools at work. It’s crucial that we, as a field, speak the same language as those implementing these tools. This is to ensure patient safety (Epic’s Sepsis cautionary tale) and to use the tools properly. They are quite good, and we should make the most of them.\n\n2.1 Understanding AI: Models vs Products\nA crucial distinction often missed is that an AI model itself is not a product. Take OpenAI as an example - while they excel at building powerful models, their success with ChatGPT comes from transforming that model into a helpful assistant. As highlighted in this brilliant Stanford talk, considering the specific context and software surrounding the model allows us to be imaginative and practical.\n\n\n2.2 The Clinical Decision Support Dilemma\nConsider clinical decision support in radiology. While companies focus on creating high-performance diagnostic models, the implementation pathway remains unclear. There is practical use in screening and translating reports for patient understanding, but clinical practice implementation remains murky.\nCurrently, using the model, the main product being created is one that generates imaging reports. Here are some options:\n\n\n2.3 Implementation Models\n\nHuman & AI Case Collaboration\n\nClinician works on the case at the same time as the AI\nThe AI report is visible for the clinician to use as desired\n\nAI-First Verification\n\nAI generates initial report\nClinician reviews and validates\n\nHuman-First Verification\n\nClinician writes initial report\nAI system performs error check\nDiscrepancies trigger senior clinician review\n\nAI as a Co-Worker\n\nAI handles routine cases & calculates confidence/complexity metrics\nComplex cases routed to senior clinician where appropriate\n\n\n\n\n\n\n\n\nCurrent Models are like GPT-4o\n\n\n\nCurrent models lack intelligent clinician-AI interaction. For instance, a very obvious to improve interaction would be to show clinicians a tree-of-thought reasoning trace for clinical reasoning transparency. As of writing this article, these are not the norm. Assume we’re talking about your run-of-the-mill GPT-4o fine-tuned on radiology data, generating reports.\n\n\nWithout sufficient thought to human-computer interaction, it’s looking pretty bleak.\nOptions 1, 2 and likely 3 cause time-poor and stressed out radiologists. Option 1’s ‘helpful’ reporter product is like a genius who sometimes gets the hardest question right and sometimes the easiest question wrong. In a healthcare setting, there is limited value - more time will be spent on all discordant cases (which may not even result in better clinical performance). Option 2 is option 1 in disguise - you risk over-reliance or ignoring useful outputs. Option 3 is more useful; it sets clear boundaries on the human-AI relationship. By only making the AI visible in discordant cases, it may serve as a good tool to ‘triage’ scans up the chain of experience. However, you run into the same ‘Who is right?’ dilemma.\nFinancially, only option 4 makes sense to radiology practices and hospitals. Ide & Talamas describe this as an autonomous agent replacing routine work, displacing humans to more specialised problem-solving. If this leads to better patient outcomes, we must choose this option. However, we also need to face significant restructuring of training programs and retrain displaced early-career specialists.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "What Medicine needs to get right about AI"
    ]
  },
  {
    "objectID": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#breaking-free-from-false-assumptions",
    "href": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#breaking-free-from-false-assumptions",
    "title": "What Medicine needs to get right about AI",
    "section": "3 Breaking Free from False Assumptions",
    "text": "3 Breaking Free from False Assumptions\nOur limited options stem from several unfortunate assumptions/starting points:\n\nOur best way to help radiologists is to diagnose for them\nThe best way to help radiologists is to write reports for them\nAI is a black box that cannot truly reason, so we can’t truly understand it\nThis means that as long as we have high-quality training data of prior reports, we can generate high-quality reports and trust them\n\nReading medical imaging itself is a process. Why can’t we have asked questions like:\n\nHow can we automatically identify and show the radiologist the key references (Radiopaedia/StatDx) they would need to look at to solve this case?\nCan we automatically show the patient’s last 5 CXRs, process them and identify exactly where changes have evolved?\nConsidering the speed of system 1 thinking, how can we best display anomaly detection with attached tree-of-thought reasoning traces while enabling a clinician’s systematic read of an image?\nDuring dictation, can we let the radiologist think out in a very unstructured manner, offering real-time reasoning feedback as well as scribing a high quality radiology report?\nCan we automate and adapt reporting for specific protocolised research guidelines?\nCan we use LLMs to enhance inter-radiologist communication to get rapid second opinions from leading experts?",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "What Medicine needs to get right about AI"
    ]
  },
  {
    "objectID": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#why-are-we-here",
    "href": "posts/What_Medicine_needs_to_get_right_about_AI/index.html#why-are-we-here",
    "title": "What Medicine needs to get right about AI",
    "section": "4 Why are we here?",
    "text": "4 Why are we here?\nOutside of a resource-poor setting, there is little unmet clinical need for an autonomous radiologist agent. The explosion of AI, the abundance of radiology reports and the monetary value in creating a high-quality autonomous agent all culminates in these foundation models that can perform exceptionally well.\nHowever, given its training with human-labelled reports and diagnoses, I question if we can truly grow in medicine with these types of models. Can we get closer to ‘perfect medicine’ by having models that talk and breathe our biases?\nHere is a direction I think would be more fruitful, we already have high-quality intelligent staff, why can’t we empower them to perform efficiently and improve to be their best? All of those 6 questions I’ve posed that aim to directly augment a radiologist’s work are tractable now. Note that they are useful products, not necessarily new models (Section 2.1).\nUnsupervised data-driven approaches can teach us so much about biomedicine - medicine will look incredibly different in the upcoming decades. We need nimble well-supported staff, with both autonomous AI and better non-autonomous copilots to maximise their clinical impact.\nWe’ll explore non-autonomous copilots and autonomous AI in more detail here including specifics of how we can think about human-AI interaction.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "What Medicine needs to get right about AI"
    ]
  },
  {
    "objectID": "posts/mAI_HCI_detailed/index.html",
    "href": "posts/mAI_HCI_detailed/index.html",
    "title": "Medical AI: Lessons on Good Human-Computer Interaction",
    "section": "",
    "text": "We can draw from a rich literature of human-computer interaction (HCI), applied automation in aviation and now emerging human-centered AI (HCAI) to think about medical AI. Many new problems we’re running into now are older problems in disguise. In this article, we’ll draw on premises from the key implementation challenges.\n\n\n\n\n\n\nKey Context\n\n\n\nThis essay builds on foundational research from Jiang et al and expands their framework with practical medical applications.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "Medical AI: Lessons on Good Human-Computer Interaction"
    ]
  },
  {
    "objectID": "posts/mAI_HCI_detailed/index.html#the-spectrum-of-ai-integration",
    "href": "posts/mAI_HCI_detailed/index.html#the-spectrum-of-ai-integration",
    "title": "Medical AI: Lessons on Good Human-Computer Interaction",
    "section": "2.1 The Spectrum of AI Integration",
    "text": "2.1 The Spectrum of AI Integration\nAI tools exist on a spectrum from autonomous to assistive:\n\nAutonomous systems like Salesforce’s Agentforce can independently handle tasks like prospect outreach\nCopilot systems like GitHub Copilot augment human work by suggesting improvements\nHybrid approaches combine both, varying by sector and risk profile\n\n\n\n\n\n\n\nImportant\n\n\n\nMost implementations will likely fall somewhere between fully autonomous and purely assistive, adapting based on the specific context and risk level of the domain.\nFor example, a medical copilot might autonomously gather drug interaction data while requiring human oversight for final dosing decisions.\n\n\nFor effective integration, well-trained staff is critical. Jeremy Howard’s new law firm, Virgil, aims for bottom-up integration of AI with staff education and AI integration from the get-go.\nSimilar to law, medicine is a field where risk is crucially minimised. Patient safety must be at the forefront. Jiang et al have identified 3 key tensions we need to solve before we can integrate AI. We’ll largely summarise their fantastic paper while applying it to medicine - please read their full paper and cite them if used.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "Medical AI: Lessons on Good Human-Computer Interaction"
    ]
  },
  {
    "objectID": "posts/mAI_HCI_detailed/index.html#a-framework-for-integration-situation-awareness",
    "href": "posts/mAI_HCI_detailed/index.html#a-framework-for-integration-situation-awareness",
    "title": "Medical AI: Lessons on Good Human-Computer Interaction",
    "section": "2.2 A Framework for Integration: Situation Awareness",
    "text": "2.2 A Framework for Integration: Situation Awareness\nBefore examining the tensions in human-AI interaction, we need to understand Situation Awareness (SA). While SA originated in aviation, it offers valuable insights into how humans and AI systems interact.\nAt its core, SA describes how humans gather and process information to solve problems. Think of it as the foundation that supports decision-making - separate from the decisions themselves.\nEven skilled decision-makers can make mistakes if their SA is incomplete or incorrect. Similarly, someone with excellent awareness might still make wrong choices due to gaps in knowledge or training.\nSA unfolds in three distinct levels:\n\nPerception: Noticing key elements in your environment within a specific time and space\nComprehension: Making sense of what these elements mean\nProjection: Anticipating how the situation might develop\n\n\n\n\nSituation Awareness\n\n\n\n2.2.1 Understanding SA Through a Medical Example\nLet’s explore how SA works in a hospital setting:\n\n2.2.1.1 Level 1: Perception\nPicture yourself on a ward round. You’re taking in everything around the patient - their appearance, vital signs, recent test results, and examination changes. You notice the electronic medical record (EMR) displaying key data, while also being aware of the broader environment: available nursing staff, family presence, and the treating team. This initial stage builds your mental model of the current situation.\n\n\n2.2.1.2 Level 2: Comprehension\nNow comes synthesis. A doctor combines all these elements into meaningful patterns. For instance, seeing low blood pressure alongside cool extremities and reduced urine output forms a clear picture of poor tissue perfusion - a gestalt that’s more meaningful than any single observation.\n\n\n2.2.1.3 Level 3: Projection and Planning\nThis is where experience and implicit learning shine. Based on the patterns recognized, a clinician can anticipate likely developments. They might predict that a patient with worsening respiratory symptoms and dropping oxygen levels will need intensive care soon, allowing them to start preparations early.\n\n\n\n2.2.2 Moving Beyond Individual SA: A Complex Systems Perspective\nTraditional SA focuses on individual decision-makers, but modern healthcare demands a broader view.\nThe Distributed Situation Awareness model recognizes that cognition is shared across a network of human agents, technological tools, and now AI systems. This creates a complex environment where the whole system’s capabilities exceed what any individual could achieve alone.\n\n\n\nDistributed Situation Awareness\n\n\nTake a modern aircraft landing. The pilot doesn’t manually calculate wind speeds, drag coefficients, or optimal flap positions. Instead, they interact with a carefully designed system that:\n\nProcesses thousands of data points about weather, aircraft weight, speed, and trajectory\nAutomatically adjusts micro-controls like precise flap angles\nPresents only the essential information the pilot needs for decision-making\nAlerts the pilot when specific actions are needed\n\nThe pilot achieves “perfect” task awareness not by knowing every calculation, but by understanding the right information at the right time. They trust the system to handle complex calculations while focusing on higher-level decisions about the landing approach.\nThis is the concept of “transactional memory”. Information is distributed across people and technology. Healthcare has always relied on distributed knowledge:\n\n2.2.2.1 Traditional Distribution\n\nClinical guidelines synthesize evidence and expertise across the field\nSubspecialists develop deep knowledge in narrow domains\nRegular updates and revisions reflect new evidence\nTeams combine different expertise for complex cases\n\n\n\n2.2.2.2 AI-Enhanced Knowledge Systems\nThe introduction of AI creates new possibilities for knowledge work:\n\nDynamic Knowledge Retrieval\n\n\nAI can instantly access and synthesize vast medical literature\nReal-time updates as new evidence emerges\nContextual retrieval based on specific patient scenarios\n\n\nKnowledge Iteration\n\n\nAI systems can test hypotheses across large datasets\nRapid identification of patterns and correlations\nContinuous learning from clinical outcomes\n\n\nKnowledge Testing\n\n\nSimulation of treatment approaches\nValidation of guidelines across diverse populations\nEarly identification of potential risks or limitations\n\n\nKnowledge Refinement\n\n\nIntegration of multiple knowledge sources\nAdaptation to local contexts and populations\nIdentification of gaps in current understanding\n\nA completely new balance emerges from AI on distributed situation awareness. AI capabilities can complement human expertise in ways previously impossible.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "Medical AI: Lessons on Good Human-Computer Interaction"
    ]
  },
  {
    "objectID": "posts/mAI_HCI_detailed/index.html#automation-vs.-human-agency",
    "href": "posts/mAI_HCI_detailed/index.html#automation-vs.-human-agency",
    "title": "Medical AI: Lessons on Good Human-Computer Interaction",
    "section": "3.1 Automation vs. Human Agency",
    "text": "3.1 Automation vs. Human Agency\n\n\n\n\n\n\nKey Takeaway\n\n\n\nYou need the right ‘window’ into the assistant AI, with the right information at the right time for a highly trained clinician.\nThis enables effective human-AI collaboration that doesn’t restrict human agency & correctly uses AI.\n\n\n\n3.1.1 The Automation Paradox in Medicine\nConsider a future where AI manages:\n\nInterpreting data-intensive multi-omic analyses\nPharmacogenomic modeling\nDrug dosing for personalized medicine\n\nBetter investigation can help us better characterise disease but this likely requires significant data analysis. This data analysis may mean greater diagnostic accuracy and treatment precision, leading to improved patient outcomes. However, the tension would arise when the intelligent system makes clinical recommendations based upon complex biomarker patterns that are opaque to the human physician and at a level of molecular detail that the human physician cannot fully process, leading to a mismatch in shared understanding between humans and AI. This negates the benefit gained from AI-powered data analysis.\nConsider even far sooner future where AI looks through the EMR and flags where treatment deviates from guidelines: The clinician needs to be trained to understand deeply how these AI-driven systems work and also how/when these systems fail. Automation bias refers to the deskilling of staff, shown to disproportionally affect non-specialist doctors in ECG interpretation. This means users need to be substantial domain experts in both medicine & applied AI to safely stay in the loop.\n\n\n3.1.2 Situation Awareness\nTo improve human agency, SA is applied as such:\n\n\n\n\n\n\n\n\nCurrent Status (Level 1 SA - Perception)\nReasoning Process (Level 2 SA - Comprehension)\nFuture Projections (Level 3 SA - Projection)\n\n\n\n\n\nShows the AI’s goals and current actions\nDisplays environmental data and performance metrics\nHelps users see what the AI sees\n\n\nExplains why the AI makes specific choices\nShows constraints affecting AI decisions\nHelps users understand AI behavior\n\n\nPredicts next steps\nHelps users see what the reasoning leads to\n\n\n\n\n\n3.1.2.1 Clinical Application: Dermatology AI Assistant\nTo illustrate these principles, consider an AI system supporting skin cancer screening:\nA dermatologist examines a patient with multiple atypical moles. The AI flags one as high-risk melanoma (89% confidence), though it doesn’t match typical patterns. Here are important considerations to retain clinician agency & augment performance.\n\n\n\n\n\n\n\n\nLevel 1: Perception\nLevel 2: Comprehension\nLevel 3: Projection\n\n\n\n\n\nClear images of the lesion\nEnhanced visualizations\nFeature detection results\n\n\nHeat maps of concerning areas\nBreakdown of each concerning feature\nSimilar cases from training data\nNotes on atypical features\n\n\nStatistical outcome predictions\nRecommended monitoring schedule\nGrowth projections\nAlternative diagnoses to consider given biopsy results\n\n\n\n\n\n\n\n\n\n\nBenefits for Physician Autonomy\n\n\n\nThis approach maintains physician control & creates effective AI-physician collaboration by critically:\n\nMaking AI decisions transparent in the way the physician thinks (AI SA supports physician SA)\nAllowing investigation of AI recommendations because fits into physician SA (AI SA supports physician SA)",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "Medical AI: Lessons on Good Human-Computer Interaction"
    ]
  },
  {
    "objectID": "posts/mAI_HCI_detailed/index.html#system-uncertainty-vs.-user-confidence",
    "href": "posts/mAI_HCI_detailed/index.html#system-uncertainty-vs.-user-confidence",
    "title": "Medical AI: Lessons on Good Human-Computer Interaction",
    "section": "3.2 System Uncertainty vs. User Confidence",
    "text": "3.2 System Uncertainty vs. User Confidence\nConsider an AI clinical decision support system. It uses all of the available multimodal data it has (e.g. all EMR data, all imaging, all relevant guidelines) and suggests next actions in a clinical workflow.\nWithout intelligently handling the known unknowns and the unknown unknowns, the system risks communicating itself with overconfidence, harming physician autonomy and trust.\n\n3.2.1 Clinical Scenario\nA 58-year-old patient presents to the Emergency Department with abdominal pain. The hospital’s AI system analyzes available data:\nEMR review:\n\nWell-controlled diabetes\nRecent normal colonoscopy\nStable vital signs\nCT scan suggesting appendicitis\nMildly elevated white blood cell count\n\nBased on these findings and clinical guidelines, the AI system confidently recommends an immediate surgical consultation for an appendicectomy/appendectomy.\nHowever, critical information remains outside the AI’s analysis:\n\n\n\n\n\n\n\nKnown gaps\nPotential unknowns\n\n\n\n\n\nRecent travel history (not in EMR)\nCurrent medication list (undocumented)\nPrecise onset timing of pain\n\n\nAnatomical variations\nUnusual pathogens\nUndocumented family history of inflammatory conditions\n\n\n\n\nThe treating physician, drawing on years of clinical experience and subtle patient cues, senses something atypical about the presentation. Rather than proceeding directly to surgery, they pursue additional workup - ultimately discovering a rare parasitic infection contracted during the patient’s recent international travel, which had mimicked appendicitis.\nThis scenario demonstrates how an AI system that doesn’t explicitly acknowledge uncertainty could prematurely narrow the diagnostic consideration and potentially override valuable physician intuition and clinical judgment. A better system would present its recommendations with appropriate caveats and confidence levels, explicitly noting what information is missing or uncertain, better supporting physician decision-making.\n\n\n3.2.2 Situation Awareness\nApplying SA in this scenario means expressing the uncertainty of the AI system. The extent to which displaying these uncertainty metrics actually improves physician confidence and decision-making remains unclear - would overall outcomes improve if it showed 65% confidence with clear knowledge gaps, versus 85% confidence without acknowledging uncertainties? Does breaking down uncertainty into specific components help or hinder clinical workflow?\nAn SA-oriented design would structure uncertainty representation across three cognitive levels, helping physicians build a complete mental model of the situation:\n\n\n\n\n\n\n\n\nLevel 1: Perception\nLevel 2: Comprehension\nLevel 3: Projection\n\n\n\n\n\nShows what data is missing from the AI’s patient model - including physical exam nuances, patient affect, and symptom progression\nDisplays standard data like CT and lab results alongside clear indicators of what information isn’t captured\nMakes gaps in the AI’s understanding explicit to help doctors put recommendations in context\n\n\nShows how different uncertainties combine to affect the AI’s confidence levels\nDemonstrates relationships between factors (e.g., how unusual pain patterns + incomplete medication history + unclear symptom timing affect diagnostic confidence)\nReveals the AI’s reasoning process rather than just showing isolated confidence scores\n\n\nEnables physicians to actively explore different diagnostic and treatment pathways while considering uncertainties\nAllows you to drill into specific concerns (like possible travel-related infections), update the AI’s reasoning to determine change in projection (“If this patient did travel to India for 3 months, what are you differentials now?”)\n\n\n\n\nThis approach recognizes that managing clinical uncertainty isn’t passive - physicians need to actively engage with the information, controlling how they view and interpret different uncertainty elements based on their expertise and the specific patient context. By organising uncertainty information around clinical goals and supporting both detailed symptom analysis and broader diagnostic consideration, the system helps physicians develop their own situation assessment while maintaining appropriate confidence in both the AI’s suggestions and their own clinical judgment.\nThis design philosophy helps harmonize the tension between the AI’s inherent limitations and the physician’s need for confident decision-making, without compromising the crucial role of clinical expertise and intuition.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "Medical AI: Lessons on Good Human-Computer Interaction"
    ]
  },
  {
    "objectID": "posts/mAI_HCI_detailed/index.html#system-complexity-vs.-perceived-complexity",
    "href": "posts/mAI_HCI_detailed/index.html#system-complexity-vs.-perceived-complexity",
    "title": "Medical AI: Lessons on Good Human-Computer Interaction",
    "section": "3.3 System Complexity vs. Perceived Complexity",
    "text": "3.3 System Complexity vs. Perceived Complexity\nThis final tension underlies the entire human-AI interaction. Modern clinical AI systems are inherently complex, integrating multiple components and data sources. In our ED scenario, the system simultaneously processes:\n\nStructured EMR data (labs, vitals, medications)\nUnstructured clinical notes\nImaging data through computer vision algorithms\nClinical guidelines and medical literature\nPopulation-level statistics and outcomes data\n\nAs the systems grows in complexity, choosing what to show to clinicians grows in importance.\nLike how micro-calculations of flap adjustment doesn’t need to be visible to the pilot, physicians need a balanced view, enough to maintain trust while preventing information overload. The operational complexity can remain manageable with a moderate perceived complexity, despite significant objective complexity.\n\n\n\n\n\n\nComplexity Definitions\n\n\n\n\nObjective Complexity: objectively what the system is doing - considering guidelines, statistics, scan results, notes, etc.\nPerceived Complexity: the complexity perceived by the clinician:\n\nWhen we distribute knowledge like trusting the pathologist to accurately read biopsy slides or the MRI machine to function properly, the perceived complexity is reduced to a simple pathology report or conversation with the radiologist.\nWe aim for reducing the perceived complexity in interdisciplinary conversations to minimise cognitive workload and maximise productivity.\n\nOperational Complexity: refers to the task’s complexity - diagnosing appendicitis may require:\n\nMultiple interdependent decisions (ordering tests, interpreting results, choosing treatments)\nSimilar presenting symptoms potentially leading to different diagnoses & various decision paths that could lead to the same diagnosis\nNeed to constantly re-evaluate as new information arrives\n\n\n\n\nThis is the hardest balance to strike. Perceived complexity changes vastly based on interface design decisions:\n\nShowing comprehensive evidence supporting the AI’s appendicitis recommendation versus overwhelming the physician with data\nBalancing immediate diagnostic suggestions with the need to communicate uncertainty\nDetermining how much of the AI’s reasoning process to expose\nAdapting information density based on case urgency and complexity\n\n\n3.3.1 Situation Awareness\nAn SA approach would manage complexity through a layered approach:\n\n\n\n\n\n\n\n\nRoutine Cases (Low Complexity):\nComplex/Atypical Cases (Medium Complexity):\nEmergent Situations (High Time Pressure):\n\n\n\n\n\nClean, simplified interface showing primary diagnosis suggestion\nKey supporting evidence summarized\nBasic confidence level displayed\nOption to explore deeper if desired\n\n\nMore detailed presentation of AI reasoning\nExplicit highlighting of uncertainty factors\nInteractive elements allowing physicians to explore different diagnostic pathways\nComparison views of similar cases and their outcomes\nVisual representation of how different factors influence the AI’s confidence\n\n\nStreamlined interface focusing on critical information\nClear action recommendations\nQuick access to essential data points\nMinimized cognitive load\n\n\n\n\nThe system should adapt its complexity presentation based on factors like:\n\nCase urgency\nPhysician experience level\nTime of day (cognitive load may be higher during night shifts)\nCase similarity to typical presentations\nLevel of diagnostic certainty\n\nThis framework recognizes that perceived complexity isn’t static - what seems straightforward at the start of a shift might feel overwhelming during a busy night. By structuring information presentation across these varying complexity levels, the system supports physicians’ natural diagnostic reasoning while maintaining their autonomy.\nFor example, in our appendicitis case:\n\nInitial presentation: Clean interface showing primary findings and straightforward recommendation\nAs atypical features emerge: System adapts to show more detailed reasoning and uncertainty factors\nIf emergency intervention needed: Interface shifts to highlight critical decision points and key actions\nDuring detailed review: Allows exploration of full case complexity for learning purposes\n\nThis SA-oriented approach helps physicians maintain situational awareness without being overwhelmed by the system’s inherent complexity, ultimately supporting better clinical decision-making while preserving physician autonomy and confidence.",
    "crumbs": [
      "Blog",
      "Recent Posts",
      "Medical AI: Lessons on Good Human-Computer Interaction"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepak_RJ",
    "section": "",
    "text": "Medical AI: Lessons on Good Human-Computer Interaction\n\n\n\n\n\n\nAI\n\n\nHCI\n\n\n\nWhat aviation can teach us about medical AI\n\n\n\n\n\nDec 29, 2024\n\n\nDeepak RJ\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Medicine needs to get right about AI\n\n\n\n\n\n\nAI\n\n\nHCI\n\n\n\nExploring the critical intersection of artificial intelligence and human-centered design in healthcare\n\n\n\n\n\nDec 24, 2024\n\n\nDeepak RJ\n\n\n\n\n\n\nNo matching items"
  }
]